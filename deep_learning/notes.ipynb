{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f3f1564",
   "metadata": {},
   "source": [
    "# General Deep Learning System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475c16e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Deep learning neural networks go through forward pass, loss function, backpropagation, gradient descent. To be specific, forward pass is to compute the output, and loss function is applied to calculate the error between labeled result and predicted result, backpropagation computes gradients of all parameter, and gradient descent to update the parameters using gradients. Through training, the model parameters are found to realize effective prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b705cdf",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf40ca",
   "metadata": {},
   "source": [
    "Deep learning models are mainly divided into Natural Language Processing (NLP) and Computer Vision (CV). \n",
    "\n",
    "Natural Language Processing focuses on analyzing and generating human language. Core NLP models include traditional sequence model such as Recurrent Neural Networks (RNNs) and LSTMs, and transformer-based model such as BERT, GPT, and T5. NLP addresses tasks like text classification, machine translation, named entity recognition, question answering, and text generation. \n",
    "\n",
    "Computer Vision focuses on understanding and interpreting visual data. Key tasks include object detection, image classification, image segmentation, and image generation. CV primarily relies on Convolutional Neural Networks (CNNs) and their variants, Vision Transformers, generative models such as Variational Autoencoders (VAE), Generative Adversarial Networks (GAN), and Diffusion Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af50f564",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be858a6",
   "metadata": {},
   "source": [
    "#### Backgroud: Why CNN?\n",
    "While Artificial Neural Networks (ANNs) introduced the fundamental structure of deep learning with input, hidden, and output layers, they face significant challenges when applied to image data. Processing high-dimensional images with fully connected layers leads to enormous computational complexity and a high risk of overfitting. To address these limitations, Convolutional Neural Networks (CNNs) were developed. Inspired by the visual processing mechanisms of the human brain, CNNs efficiently capture spatial hierarchies and local patterns in images while dramatically reducing the number of parameters, making them well-suited for tasks such as image recognition, object detection, and other computer vision applications.\n",
    "\n",
    "#### Architecture: What is CNN?\n",
    "\n",
    "The architecture of a **Convolutional Neural Network (CNN)** typically consists of the following components:\n",
    "\n",
    "1. **Input Layer**  \n",
    "   Accepts raw data, such as images, usually represented as height × width × channels.\n",
    "\n",
    "2. **Convolutional Layer**  \n",
    "   Applies multiple **filters/kernels** to extract local features like edges, textures, or patterns.\n",
    "\n",
    "3. **Pooling Layer**  \n",
    "   Reduces the spatial dimensions of feature maps (e.g., using max or average pooling), which decreases computation and helps prevent overfitting.\n",
    "\n",
    "4. **Fully Connected Layer**  \n",
    "   Connects all neurons from the previous layer to produce high-level representations and enables classification.\n",
    "\n",
    "5. **Output Layer**  \n",
    "   Provides the final prediction, often using a **softmax** or **sigmoid** activation depending on the task.\n",
    "\n",
    "#### Application: What are the applications of variant CNNs?\n",
    "\n",
    "- **LeNet** – One of the earliest CNNs, designed for **handwritten digit recognition** (MNIST dataset).  \n",
    "- **AlexNet** – Popularized deep CNNs; used for **large-scale image classification** (ImageNet), introduced **ReLU and dropout** for better training.  \n",
    "- **VGG** – Focuses on **deep but simple architectures** with small 3×3 filters, widely used for **image classification and feature extraction**.  \n",
    "- **ResNet** – Introduces **residual connections** to train very deep networks efficiently, excels in **image classification and recognition tasks**.  \n",
    "- **DenseNet** – Connects each layer to every other layer, improving **feature reuse and gradient flow**, used for **image classification and segmentation**.  \n",
    "- **U-Net** – Designed for **image segmentation**, especially in **medical imaging**, with a contracting and expanding path to capture context and precise localization.  \n",
    "- **Faster R-CNN** – Designed for **object detection**, combining region proposal networks with CNNs to efficiently detect and classify objects in images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04de811",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2846f064",
   "metadata": {},
   "source": [
    "#### Background: Why RNN?  \n",
    "While CNNs excel at capturing spatial patterns in images, they are not naturally suited for sequential data, where order and temporal dependencies matter. Standard feedforward networks (including CNNs) process inputs independently and cannot retain context across time steps. To address this limitation, **Recurrent Neural Networks (RNNs)** were developed, which introduce **loops in their architecture**, allowing information to persist across steps. This makes them well-suited for tasks such as natural language processing, speech recognition, and time-series prediction.  \n",
    "\n",
    "\n",
    "#### Architecture: What is RNN?  \n",
    "The architecture of a **Recurrent Neural Network (RNN)** typically consists of the following components:\n",
    "\n",
    "1. **Input Layer**  \n",
    "   Accepts sequential data, such as words, audio signals, or time-series measurements. The input at each time step can be a vector representing features or embeddings.\n",
    "\n",
    "2. **Hidden / Recurrent Layer**  \n",
    "   Maintains a **hidden state** that captures information from previous time steps. At each step, the network updates this state using both the current input and the previous hidden state.\n",
    "\n",
    "3. **Output Layer**  \n",
    "   Produces predictions at each time step (many-to-many) or a single prediction after the sequence (many-to-one), depending on the task. Activation functions such as **softmax** or **sigmoid** are commonly used.\n",
    "\n",
    "\n",
    "#### Applications: What are the applications of variant RNNs?\n",
    "\n",
    "- **Vanilla RNN** – The basic RNN unit; used for **short-sequence modeling**, like small time-series tasks.  \n",
    "- **LSTM (Long Short-Term Memory)** – Solves the **vanishing gradient problem**, capturing **long-term dependencies** in sequences; widely used in **language modeling, speech recognition, and time-series forecasting**.  \n",
    "- **GRU (Gated Recurrent Unit)** – A simplified LSTM with fewer parameters; balances **efficiency and performance**, used in **sequence prediction and translation**.  \n",
    "- **Bidirectional RNN** – Processes sequences in both forward and backward directions, improving context understanding; used in **NER, speech, and text processing**.  \n",
    "- **Attention-based RNNs** – Introduces **attention mechanisms** to focus on important parts of the sequence, enhancing performance in **translation and summarization**.  \n",
    "- **Seq2Seq (Encoder-Decoder RNN)** – Converts an input sequence to an output sequence, commonly used in **machine translation, chatbot response generation, and summarization**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb9682c",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2cda6e",
   "metadata": {},
   "source": [
    "#### Background: Why Transformer?  \n",
    "While RNN and its variants can realize context awareness with sequential structure, they often struggle to remember relevant data from pervious time steps when handling long sequences and be very computational expensive. The **Attention** mechanism allows the model to weigh the importance of different positions in the sequence, enabling parallel computation and better handling of long sequences. Transformer-based models have achieved state-of-the-art performance in a variety of tasks, including reading comprehension, abstractive summarization, machine translation, and language modeling.\n",
    "\n",
    "#### Architecture: What is Transformer?  \n",
    "The architecture of a **Transformer** typically consists of the following components:\n",
    "\n",
    "1. **Input Layer**  \n",
    "   Accepts sequential data, such as words, audio signals, or time-series measurements. Each input token is usually converted into a vector embedding, often enriched with positional encodings to perserve order information, since Transformers are inherently order-agnostic.\n",
    "\n",
    "2. **Encoder Stacks**\n",
    "Each encoder layer consists of \n",
    "* Multi-head Self-Attention: Captures dependencies between all positions in the sequence simultaneously\n",
    "* Feed-Forward Network (FFN): Applies non-linear transformations to each position independently\n",
    "* Residual Connections & Layer Norm: Stabilizes training and improves gradient flow\n",
    "   \n",
    "3. **Decoder Stacks** \n",
    "Each decoder layer consists of \n",
    "* Masked Multi-head Self-Attention: Ensures the model cannot 'see the future' tokens during training\n",
    "* Encoder-Decoder Attention: Allows the decoder to attend to the encoder's output\n",
    "* Feed-Forward Network (FFN): Applies non-linear transformations to each position independently\n",
    "* Residual Connections & Layer Norm: Stabilizes training and improves gradient flow\n",
    "   \n",
    "4. **Attention**\n",
    "* Self-Attention: Determines the relevance of other positions in the sequence to a given token\n",
    "* Scaled Dot-Product Attention: Computes attention weights efficiently by scaling the dot product of quries and keys\n",
    "* Multi-Head Attention: Allows the model to attend to information from multiple representation subspaces simultaneously\n",
    "  \n",
    "5. **Output Layer**\n",
    "Produce the final prediction\n",
    "\n",
    "\n",
    "\n",
    "##### Technical Details: Attention\n",
    "Attention can be described as mapping a query and a set of key-value pairs to an output, where the query, key, values and output are all vectors. The output is computed as a weighted sum of the values.\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49fbec",
   "metadata": {},
   "source": [
    "#### Applications: What are the applications of variant Transformer?\n",
    "Transformer variants have been adapted for many domains beyond natural language processing:\n",
    "\n",
    "1. **NLP**\n",
    "* Machine Translation (e.g., Google Translate)\n",
    "* Abstractive Summarization \n",
    "* Question Answering / Reading Comprehension\n",
    "* Language Modeling (e.g., GPT, BERT, T5)\n",
    "\n",
    "2. **CV**\n",
    "* Vision Transformers (ViT) for image classification\n",
    "* Object Detection (DETR)\n",
    "* Image Generation and Super-Resolution\n",
    "* Speech Recognition (e.g., Speech-Transformer)\n",
    "* Text-to-Speech synthesis\n",
    "\n",
    "3. **Reinforcement Learning**\n",
    "* Decision-making policies with sequential context\n",
    "* Multi-modal Transformers that handle text, image, and audio simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e788d6",
   "metadata": {},
   "source": [
    "#### Reference\n",
    "https://nlp.seas.harvard.edu/annotated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeff51b",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbbd3df",
   "metadata": {},
   "source": [
    "\n",
    "Activation functions introduce **non-linearity** into neural networks, enabling them to learn complex patterns. Here are some common ones:\n",
    "\n",
    "\n",
    "#### 1. ReLU (Rectified Linear Unit)\n",
    "- **Formula:** `f(x) = max(0, x)`\n",
    "- **Range:** [0, ∞)\n",
    "- **Pros:** Simple, fast, reduces vanishing gradient\n",
    "- **Cons:** Dead neurons if input < 0 always\n",
    "- **Use case:** Hidden layers in CNNs, RNNs, Transformers\n",
    "\n",
    "\n",
    "#### 2. Leaky ReLU\n",
    "- **Formula:** `f(x) = x if x >= 0 = α * x if x < 0 (usually α = 0.01)`\n",
    "- **Range:** (-∞, ∞)\n",
    "- **Pros:** Solves “dead neuron” problem\n",
    "- **Cons:** Slightly more computation than ReLU\n",
    "- **Use case:** Hidden layers where standard ReLU fails\n",
    "\n",
    "#### 3. Sigmoid\n",
    "- **Formula:** `f(x) = 1 / (1 + exp(-x))`\n",
    "- **Range:** (0, 1)\n",
    "- **Pros:** Output interpretable as probability\n",
    "- **Cons:** Vanishing gradients for large |x|, not centered at 0\n",
    "- **Note:** Sigmoid is for binary classification, softmax is for n-class classification\n",
    "\n",
    "\n",
    "#### 4. Tanh (Hyperbolic Tangent)\n",
    "- **Formula:** `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
    "- **Range:** (-1, 1)\n",
    "- **Pros:** Centered at 0 → better convergence than sigmoid\n",
    "- **Cons:** Still suffers from vanishing gradient\n",
    "- **Use case:** Hidden layers in older RNNs or small networks\n",
    "\n",
    "\n",
    "#### 5. GELU (Gaussian Error Linear Unit)\n",
    "- **Formula:** `f(x) = x * P(X <= x), X ~ N(0,1)` (approx: `x * sigmoid(1.702 * x)`)\n",
    "- **Range:** (-∞, ∞)\n",
    "- **Pros:** Smooth, modern, works well in Transformers\n",
    "- **Cons:** Slightly more computationally expensive\n",
    "- **Use case:** Hidden layers in Transformers, modern deep networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9eed14",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993cfa3f",
   "metadata": {},
   "source": [
    "Loss functions in deep learning are introduced to quantify how wrong a model’s predictions are, guiding optimization to improve performance.\n",
    "\n",
    "\n",
    "#### 1. Regression Losses\n",
    "Used for predicting **continuous values**.\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Measures squared difference between prediction and target. Penalizes large errors heavily.\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "\n",
    "- **Mean Absolute Error (MAE)**: Measures absolute difference. Less sensitive to outliers.\n",
    "$$\n",
    "\\text{MAE} = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "- **Huber Loss**: Combines MSE and MAE. Smooth near zero, robust to outliers.\n",
    "\n",
    "$$\n",
    "\\text{Huber}(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2}(y - \\hat{y})^2 & \\text{if } |y - \\hat{y}| \\le \\delta \\\\\n",
    "\\delta (|y - \\hat{y}| - \\frac{\\delta}{2}) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### 2. Classification Losses\n",
    "Used for predicting **discrete classes**.\n",
    "\n",
    "- **Binary Cross-Entropy (BCE)**\n",
    "$$\n",
    "\\text{BCE} = - \\frac{1}{N} \\sum_{i=1}^N [ y_i \\log \\hat{y}_i + (1 - y_i)\\log(1 - \\hat{y}_i) ]\n",
    "$$\n",
    "\n",
    "For **binary classification**. Works with **sigmoid outputs**.\n",
    "\n",
    "- **Categorical Cross-Entropy (CCE)**\n",
    "$$\n",
    "\\text{CCE} = - \\sum_{i=1}^N \\sum_{c=1}^C y_{i,c} \\log \\hat{y}_{i,c}\n",
    "$$\n",
    "\n",
    "For **multi-class classification**. Works with **softmax outputs**.\n",
    "\n",
    "- **Kullback-Leibler Divergence (KL Divergence)**\n",
    "$$\n",
    "\\text{KL}(P || Q) = \\sum_i P(i) \\log \\frac{P(i)}{Q(i)}\n",
    "$$\n",
    "Measures the difference between two probability distributions. Used in **VAEs** and **knowledge distillation**.\n",
    "\n",
    "\n",
    "#### 3. Margin-Based / Ranking Losses\n",
    "\n",
    "- **Hinge Loss**\n",
    "$$\n",
    "\\text{Hinge} = \\max(0, 1 - y \\cdot \\hat{y})\n",
    "$$\n",
    "Used in **SVMs**. Encourages predictions to respect a margin.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d46734a",
   "metadata": {},
   "source": [
    "## Optimization Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3628aae2",
   "metadata": {},
   "source": [
    "Backproprogation is to use the chain rule to calculate the gradients of loss w.r.t all parameters in the model, and gradient descent is to use gradient to update parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb55c5",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "* Batch gradient descent: compute the gradient over the whole dataset before updating parameters\n",
    "* Stochastic gradient descent: shuffle the dataset, randomly pick one sample from the dataset, compute the gradient for the selected sample and update the parameter iteratively through the whole dataset\n",
    "* Mini-batches: shuffle the dataset, randomly pick a mini batch samples from the dataset, compute the gradient for the mini batch and update the parameter iteratively through the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e4f9a6",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "- **Purpose:** Smooth gradient updates and accelerate convergence along consistent directions.  \n",
    "- **Mechanism:** Introduces a velocity term that accumulates past gradients. \n",
    "$$\n",
    "v_{t+1} = \\beta v_t + (1 - \\beta)\\nabla _{\\theta} L(\\theta_t) \\\\\n",
    "\\theta_{t+1} = \\theta_t - \\eta v_{t+1}\n",
    "$$\n",
    "- **Effect:** Speeds up convergence in shallow valleys and reduces oscillations in steep directions\n",
    "\n",
    "### RMSProp\n",
    "- **Purpose:** Adaptively scale the learning rate for each parameter to stabilize updates.  \n",
    "- **Mechanism:** Maintain a running average of squared gradients \n",
    "$$\n",
    "E[g^2]_t = \\beta E[g^2]_{t-1} + (1 - \\beta)(\\nabla _{\\theta} L)^2\\\\\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{E[g^2] + \\epsilon} \\nabla _{\\theta} L\n",
    "$$\n",
    "- **Effect:** Slow down updates in directions with large gradients and speed up updates in directions with small gradients\n",
    "\n",
    "### Adam\n",
    "- **Purpose:** Combines Momentum and RMSProp.  \n",
    "- **Mechanism:** Maintain a running average of squared gradients \n",
    "- **Effect:** Fast convergence, handles noisy or sparse gradients well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f866b1",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "* weight decay\n",
    "  * L1 (Ridge)\n",
    "    L1 Regularization adds a proportional to the absolute value of the weights, in order to encourage sparsity and thereby reducing its complexity and variance. Since L1 zeros out unimportant feature and L2 shrinks all features, L1 is more useful than L2 in feature selection.\n",
    "    $$\n",
    "    Loss_{L1} = Original Loss + \\lambda \\Sigma_{i} |w_i|\n",
    "    $$\n",
    "  * L2 (Lasso)\n",
    "    L2 Regularization adds a proportional to the square of the weights, in order to encourage smaller weights and smooth the model predictions.\n",
    "    $$\n",
    "    Loss_{L2} = Original Loss + \\lambda \\Sigma_{i} w_i^2\n",
    "    $$\n",
    "\n",
    "* parameter sharing: By reducing number of free parameters, thereby reducing variance (less overfitting) and enforce temporal consistency\n",
    "  \n",
    "* model averaging: combine predictions from multiple models to reduce variance (make predictions more stable) and bring better generalization.\n",
    "* dropout: stochastically set some of outputs to zero with probability p to force the network not reply on specific neurons.\n",
    "* learning curves: plot training and validation performance to help diagnoise underfitting and overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf3bda",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "* Data Normalization: Input data are preprocessed to follow zero mean and unit variance before the training.\n",
    "* Batch Normalization: To avoid gradient explosion or vanishing and make learning easier for the next layer, each batch before activation function is preprocessed to follow zero mean and unit variance for each feature per layer.\n",
    "* Layer Norm: To avoid gradient explosion or vanishing and make learning easier for the next layer, each sample before activation function is preprocessed to follow zero mean and unit variance for each sample per layer, working independently of batch size."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
